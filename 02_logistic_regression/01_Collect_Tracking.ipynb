{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T22:52:22.961939Z",
     "start_time": "2020-09-20T22:52:12.203778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.2.10:4040\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1600642335959)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.hadoopConfiguration.set(\"fs.s3n.awsAccessKeyId\",sys.env(\"AWS_ACCESS_KEY\"))\n",
    "sc.hadoopConfiguration.set(\"fs.s3n.awsSecretAccessKey\", sys.env(\"AWS_SECRET_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T22:52:23.463462Z",
     "start_time": "2020-09-20T22:52:22.963883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(ip,StringType,true), StructField(app,StringType,true), StructField(device,StringType,true), StructField(os,StringType,true), StructField(channel,StringType,true), StructField(click_time,StringType,true), StructField(attributed_time,StringType,true), StructField(is_attributed,StringType,true))\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val schema = StructType(\n",
    "  Array(\n",
    "    StructField(\"ip\", StringType, true),\n",
    "    StructField(\"app\", StringType, true),\n",
    "    StructField(\"device\", StringType, true),\n",
    "    StructField(\"os\", StringType, true),\n",
    "    StructField(\"channel\", StringType, true),\n",
    "    StructField(\"click_time\", StringType, true),\n",
    "    StructField(\"attributed_time\", StringType, true),\n",
    "    StructField(\"is_attributed\", StringType, true) \n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T22:52:35.741424Z",
     "start_time": "2020-09-20T22:52:23.467230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+-------+-------------------+-------------------+-------------+\n",
      "|    ip|app|device| os|channel|         click_time|    attributed_time|is_attributed|\n",
      "+------+---+------+---+-------+-------------------+-------------------+-------------+\n",
      "| 78377|  3|     1| 19|    280|2017-11-07 02:03:23|               null|            0|\n",
      "| 92057| 14|     1|149|    123|2017-11-07 02:03:23|               null|            0|\n",
      "| 26726|  3|     1| 19|    280|2017-11-07 02:03:23|               null|            0|\n",
      "|114083|  9|     1| 22|    134|2017-11-07 02:03:23|               null|            0|\n",
      "|115621| 35|     1| 19|     21|2017-11-07 02:03:23|2017-11-07 02:26:17|            1|\n",
      "|130792| 15|     1| 18|    245|2017-11-07 02:03:23|               null|            0|\n",
      "|100275| 15|     1| 19|      3|2017-11-07 02:03:23|               null|            0|\n",
      "|126918| 19|     0| 24|    213|2017-11-07 02:03:23|2017-11-07 11:24:28|            1|\n",
      "| 98365| 18|     1| 13|    107|2017-11-07 02:03:23|               null|            0|\n",
      "|  5348|  8|     1| 19|    145|2017-11-07 02:03:23|               null|            0|\n",
      "+------+---+------+---+-------+-------------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// data source: https://www.kaggle.com/matleonard/feature-engineering-data\n",
    "spark.read.format(\"csv\")\n",
    "    .schema(schema)\n",
    "    .load(\"s3n://renan-test/adtracking/*.csv\")\n",
    "    .limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T23:37:19.650371Z",
     "start_time": "2020-09-20T22:52:35.743181Z"
    }
   },
   "outputs": [
    {
     "ename": "org.apache.spark.sql.streaming.StreamingQueryException",
     "evalue": " Query [id = c9b8dd27-4d0c-4577-bea7-428e228e1296, runId = c5611f4e-9e9e-44b8-92c4-fe244b52edb5] terminated with exception: Read timed out",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.streaming.StreamingQueryException: Query [id = c9b8dd27-4d0c-4577-bea7-428e228e1296, runId = c5611f4e-9e9e-44b8-92c4-fe244b52edb5] terminated with exception: Read timed out",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:355)",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)",
      "Caused by: java.net.SocketTimeoutException: Read timed out",
      "  at java.net.SocketInputStream.socketRead0(Native Method)",
      "  at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)",
      "  at java.net.SocketInputStream.read(SocketInputStream.java:171)",
      "  at java.net.SocketInputStream.read(SocketInputStream.java:141)",
      "  at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)",
      "  at sun.security.ssl.InputRecord.read(InputRecord.java:503)",
      "  at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:990)",
      "  at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:948)",
      "  at sun.security.ssl.AppInputStream.read(AppInputStream.java:105)",
      "  at org.apache.http.impl.io.AbstractSessionInputBuffer.fillBuffer(AbstractSessionInputBuffer.java:161)",
      "  at org.apache.http.impl.io.SocketInputBuffer.fillBuffer(SocketInputBuffer.java:82)",
      "  at org.apache.http.impl.io.AbstractSessionInputBuffer.readLine(AbstractSessionInputBuffer.java:276)",
      "  at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:138)",
      "  at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:56)",
      "  at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:259)",
      "  at org.apache.http.impl.AbstractHttpClientConnection.receiveResponseHeader(AbstractHttpClientConnection.java:294)",
      "  at org.apache.http.impl.conn.DefaultClientConnection.receiveResponseHeader(DefaultClientConnection.java:257)",
      "  at org.apache.http.impl.conn.AbstractClientConnAdapter.receiveResponseHeader(AbstractClientConnAdapter.java:230)",
      "  at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273)",
      "  at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)",
      "  at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:684)",
      "  at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:486)",
      "  at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:835)",
      "  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)",
      "  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)",
      "  at org.jets3t.service.impl.rest.httpclient.RestStorageService.performRequest(RestStorageService.java:328)",
      "  at org.jets3t.service.impl.rest.httpclient.RestStorageService.performRequest(RestStorageService.java:279)",
      "  at org.jets3t.service.impl.rest.httpclient.RestStorageService.performRestHead(RestStorageService.java:1052)",
      "  at org.jets3t.service.impl.rest.httpclient.RestStorageService.getObjectImpl(RestStorageService.java:2264)",
      "  at org.jets3t.service.impl.rest.httpclient.RestStorageService.getObjectDetailsImpl(RestStorageService.java:2193)",
      "  at org.jets3t.service.StorageService.getObjectDetails(StorageService.java:1120)",
      "  at org.jets3t.service.StorageService.getObjectDetails(StorageService.java:575)",
      "  at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:174)",
      "  at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)",
      "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "  at java.lang.reflect.Method.invoke(Method.java:498)",
      "  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)",
      "  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
      "  at org.apache.hadoop.fs.s3native.$Proxy14.retrieveMetadata(Unknown Source)",
      "  at org.apache.hadoop.fs.s3native.NativeS3FileSystem.listStatus(NativeS3FileSystem.java:530)",
      "  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.listLeafFiles(InMemoryFileIndex.scala:322)",
      "  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.$anonfun$bulkListLeafFiles$1(InMemoryFileIndex.scala:195)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:238)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:187)",
      "  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:135)",
      "  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)",
      "  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)",
      "  at org.apache.spark.sql.execution.streaming.FileStreamSource.allFilesUsingInMemoryFileIndex(FileStreamSource.scala:209)",
      "  at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchAllFiles(FileStreamSource.scala:262)",
      "  at org.apache.spark.sql.execution.streaming.FileStreamSource.fetchMaxOffset(FileStreamSource.scala:122)",
      "  at org.apache.spark.sql.execution.streaming.FileStreamSource.latestOffset(FileStreamSource.scala:286)",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:380)",
      "  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)",
      "  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:371)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)",
      "  at scala.collection.immutable.Map$Map1.foreach(Map.scala:128)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:238)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:368)",
      "  at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)",
      "  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)",
      "  at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "val trackingStream =  spark.readStream.format(\"csv\")\n",
    "    .schema(schema)\n",
    "    .load(\"s3n://renan-test/adtracking/*.csv\")\n",
    "    .withColumn(\"value\",to_json(struct($\"ip\",$\"app\",$\"device\",$\"os\",$\"channel\",$\"click_time\",$\"attributed_time\",$\"is_attributed\")))\n",
    "    .withColumn(\"key\",$\"app\")\n",
    "    .select(\"key\",\"value\")\n",
    "\n",
    "trackingStream.writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"topic\", \"tracking\")\n",
    "    .option(\"checkpointLocation\", \"/home/rhuanca/demo/_checkpoints/tracking\")\n",
    "    .start()\n",
    "    .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
