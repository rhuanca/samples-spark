{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T13:14:10.664765Z",
     "start_time": "2020-09-27T13:14:02.024276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.2.10:4041\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1601212444714)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "init: Int = 1\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val init = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T13:14:10.901207Z",
     "start_time": "2020-09-27T13:14:10.668478Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.feature.OneHotEncoder\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding (Classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T13:14:14.935317Z",
     "start_time": "2020-09-27T13:14:10.904187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+\n",
      "| X1| X2| X3| X4| X5| X6|\n",
      "+---+---+---+---+---+---+\n",
      "|  v|  n|  f|  d|  t|  a|\n",
      "|  b| ai|  a|  d|  b|  g|\n",
      "|  v| as|  f|  d|  a|  j|\n",
      "|  l|  n|  f|  d|  z|  l|\n",
      "|  s| as|  c|  d|  y|  i|\n",
      "| aa| ai|  e|  d|  x|  g|\n",
      "|  b| ae|  d|  d|  x|  d|\n",
      "|  s| ae|  c|  d|  h|  d|\n",
      "|  l|  s|  c|  d|  h|  j|\n",
      "|  v| as|  f|  d|  g|  f|\n",
      "|  l|  s|  c|  d|  g|  d|\n",
      "|  b|  b|  a|  d|  g|  l|\n",
      "|  r|  e|  f|  d|  g|  h|\n",
      "|  v| ae|  g|  d|  g|  g|\n",
      "|  a| ak|  f|  d|  g|  l|\n",
      "|  r|  e|  f|  d|  g|  h|\n",
      "|  l|  s|  c|  d|  g|  d|\n",
      "|  r|  e|  f|  d|  f|  h|\n",
      "|  i| ai|  a|  d|  f|  l|\n",
      "|  r| as|  f|  d|  f|  d|\n",
      "+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .load(\"mercedes-benz-greener-manufacturing_test.csv\")\n",
    "    .select(\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\")\n",
    "    .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T13:14:17.685895Z",
     "start_time": "2020-09-27T13:14:14.937734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+---------+---------+\n",
      "|count(X1)|count(X2)|count(X2)|count(X3)|count(X5)|count(X6)|\n",
      "+---------+---------+---------+---------+---------+---------+\n",
      "|       27|       45|       45|        7|       32|       12|\n",
      "+---------+---------+---------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// counting how many there are for every columns\n",
    "spark.read.format(\"csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .load(\"mercedes-benz-greener-manufacturing_test.csv\")\n",
    "    .select(\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\")\n",
    "    .agg(countDistinct(\"X1\"),countDistinct(\"X2\"),countDistinct(\"X2\"),countDistinct(\"X3\"),countDistinct(\"X5\"),countDistinct(\"X6\"))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T13:14:18.398849Z",
     "start_time": "2020-09-27T13:14:17.687779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| X1| X5|\n",
      "+---+---+\n",
      "|  v|  t|\n",
      "|  b|  b|\n",
      "|  v|  a|\n",
      "|  l|  z|\n",
      "|  s|  y|\n",
      "| aa|  x|\n",
      "|  b|  x|\n",
      "|  s|  h|\n",
      "|  l|  h|\n",
      "|  v|  g|\n",
      "|  l|  g|\n",
      "|  b|  g|\n",
      "|  r|  g|\n",
      "|  v|  g|\n",
      "|  a|  g|\n",
      "|  r|  g|\n",
      "|  l|  g|\n",
      "|  r|  f|\n",
      "|  i|  f|\n",
      "|  r|  f|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "manufacturing: org.apache.spark.sql.DataFrame = [X1: string, X5: string]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val manufacturing = spark.read.format(\"csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .load(\"mercedes-benz-greener-manufacturing_test.csv\")\n",
    "    .select(\"X1\",\"X5\")\n",
    "\n",
    "manufacturing.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T13:14:19.635564Z",
     "start_time": "2020-09-27T13:14:18.400873Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+--------+\n",
      "|X1 |X1_index|X5 |X5_index|\n",
      "+---+--------+---+--------+\n",
      "|v  |4.0     |t  |29.0    |\n",
      "|b  |3.0     |b  |28.0    |\n",
      "|v  |4.0     |a  |27.0    |\n",
      "|l  |2.0     |z  |31.0    |\n",
      "|s  |1.0     |y  |30.0    |\n",
      "|aa |0.0     |x  |26.0    |\n",
      "|b  |3.0     |x  |26.0    |\n",
      "|s  |1.0     |h  |25.0    |\n",
      "|l  |2.0     |h  |25.0    |\n",
      "|v  |4.0     |g  |23.0    |\n",
      "+---+--------+---+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_a24afce31056\n",
       "indexed: org.apache.spark.sql.DataFrame = [X1: string, X5: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexer = new StringIndexer()\n",
    "    .setInputCols(Array(\"X1\",\"X5\"))\n",
    "    .setOutputCols(Array(\"X1_index\",\"X5_index\"))\n",
    "\n",
    "val indexed = indexer.fit(manufacturing).transform(manufacturing)\n",
    "indexed\n",
    "    .select(\"X1\",\"X1_index\",\"X5\",\"X5_index\")\n",
    "    .show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T13:14:20.322618Z",
     "start_time": "2020-09-27T13:14:19.642971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+---+--------+---------------+\n",
      "|X1 |X1_index|X1_vec        |X5 |X5_index|X5_vec         |\n",
      "+---+--------+--------------+---+--------+---------------+\n",
      "|v  |4.0     |(26,[4],[1.0])|t  |29.0    |(31,[29],[1.0])|\n",
      "|b  |3.0     |(26,[3],[1.0])|b  |28.0    |(31,[28],[1.0])|\n",
      "|v  |4.0     |(26,[4],[1.0])|a  |27.0    |(31,[27],[1.0])|\n",
      "|l  |2.0     |(26,[2],[1.0])|z  |31.0    |(31,[],[])     |\n",
      "|s  |1.0     |(26,[1],[1.0])|y  |30.0    |(31,[30],[1.0])|\n",
      "|aa |0.0     |(26,[0],[1.0])|x  |26.0    |(31,[26],[1.0])|\n",
      "|b  |3.0     |(26,[3],[1.0])|x  |26.0    |(31,[26],[1.0])|\n",
      "|s  |1.0     |(26,[1],[1.0])|h  |25.0    |(31,[25],[1.0])|\n",
      "|l  |2.0     |(26,[2],[1.0])|h  |25.0    |(31,[25],[1.0])|\n",
      "|v  |4.0     |(26,[4],[1.0])|g  |23.0    |(31,[23],[1.0])|\n",
      "+---+--------+--------------+---+--------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHotEncoder_d00c1e2883a8\n",
       "encoded: org.apache.spark.sql.DataFrame = [X1: string, X5: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val encoder = new OneHotEncoder()\n",
    "    .setInputCols(Array(\"X1_index\",\"X5_index\"))\n",
    "    .setOutputCols(Array(\"X1_vec\",\"X5_vec\"))\n",
    "\n",
    "val encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "encoded\n",
    "    .select(\"X1\",\"X1_index\",\"X1_vec\",\"X5\",\"X5_index\",\"X5_vec\")\n",
    "    .show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding of Top 10 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T13:14:21.891048Z",
     "start_time": "2020-09-27T13:14:20.326813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| X1|count|\n",
      "+---+-----+\n",
      "| aa|  826|\n",
      "|  s|  602|\n",
      "|  l|  599|\n",
      "|  b|  596|\n",
      "|  v|  436|\n",
      "|  r|  252|\n",
      "|  i|  189|\n",
      "|  a|  153|\n",
      "|  c|  142|\n",
      "|  o|   81|\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+\n",
      "| X5|count|\n",
      "+---+-----+\n",
      "|  v|  246|\n",
      "|  r|  239|\n",
      "|  p|  227|\n",
      "|  w|  218|\n",
      "| af|  217|\n",
      "| ad|  213|\n",
      "| ac|  212|\n",
      "|  n|  209|\n",
      "|  l|  206|\n",
      "|  s|  205|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "X1_top10: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [X1: string, count: bigint]\n",
       "X5_top10: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [X5: string, count: bigint]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val X1_top10 = manufacturing.groupBy(\"X1\").count.orderBy(desc(\"count\")).limit(10)\n",
    "val X5_top10 = manufacturing.groupBy(\"X5\").count.orderBy(desc(\"count\")).limit(10)\n",
    "\n",
    "X1_top10.show\n",
    "X5_top10.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T13:14:23.234301Z",
     "start_time": "2020-09-27T13:14:21.894468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top10_X1_categories: Array[Any] = Array(aa, s, l, b, v, r, i, a, c, o)\n",
       "top10_X5_categories: Array[Any] = Array(v, r, p, w, af, ad, ac, n, l, s)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val top10_X1_categories = X1_top10.select(\"X1\").collect.map(_(0))\n",
    "val top10_X5_categories = X5_top10.select(\"X5\").collect.map(_(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T13:14:23.690143Z",
     "start_time": "2020-09-27T13:14:23.236837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-----+\n",
      "| X1|X1_v2| X5|X5_v2|\n",
      "+---+-----+---+-----+\n",
      "|  v|    v|  t|    0|\n",
      "|  b|    b|  b|    0|\n",
      "|  v|    v|  a|    0|\n",
      "|  l|    l|  z|    0|\n",
      "|  s|    s|  y|    0|\n",
      "| aa|   aa|  x|    0|\n",
      "|  b|    b|  x|    0|\n",
      "|  s|    s|  h|    0|\n",
      "|  l|    l|  h|    0|\n",
      "|  v|    v|  g|    0|\n",
      "|  l|    l|  g|    0|\n",
      "|  b|    b|  g|    0|\n",
      "|  r|    r|  g|    0|\n",
      "|  v|    v|  g|    0|\n",
      "|  a|    a|  g|    0|\n",
      "|  r|    r|  g|    0|\n",
      "|  l|    l|  g|    0|\n",
      "|  r|    r|  f|    0|\n",
      "|  i|    i|  f|    0|\n",
      "|  r|    r|  f|    0|\n",
      "+---+-----+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "manufacturing_v2: org.apache.spark.sql.DataFrame = [X1: string, X1_v2: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val manufacturing_v2 = manufacturing\n",
    "    .withColumn(\"X1_v2\",when($\"X1\".isin(top10_X1_categories:_*),$\"X1\").otherwise(lit(\"0\")))\n",
    "    .withColumn(\"X5_v2\",when($\"X5\".isin(top10_X5_categories:_*),$\"X5\").otherwise(lit(\"0\")))\n",
    "    .select(\"X1\",\"X1_v2\",\"X5\",\"X5_v2\")\n",
    "\n",
    "manufacturing_v2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T13:14:24.343895Z",
     "start_time": "2020-09-27T13:14:23.692792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----+--------+\n",
      "|X1_v2|X1_index|X5_v2|X5_index|\n",
      "+-----+--------+-----+--------+\n",
      "|    v|     0.0|    0|    10.0|\n",
      "|    b|     7.0|    0|    10.0|\n",
      "|    v|     0.0|    0|    10.0|\n",
      "|    l|     4.0|    0|    10.0|\n",
      "|    s|     1.0|    0|    10.0|\n",
      "|   aa|     8.0|    0|    10.0|\n",
      "|    b|     7.0|    0|    10.0|\n",
      "|    s|     1.0|    0|    10.0|\n",
      "|    l|     4.0|    0|    10.0|\n",
      "|    v|     0.0|    0|    10.0|\n",
      "|    l|     4.0|    0|    10.0|\n",
      "|    b|     7.0|    0|    10.0|\n",
      "|    r|     2.0|    0|    10.0|\n",
      "|    v|     0.0|    0|    10.0|\n",
      "|    a|     9.0|    0|    10.0|\n",
      "|    r|     2.0|    0|    10.0|\n",
      "|    l|     4.0|    0|    10.0|\n",
      "|    r|     2.0|    0|    10.0|\n",
      "|    i|     5.0|    0|    10.0|\n",
      "|    r|     2.0|    0|    10.0|\n",
      "+-----+--------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "indexer_v2: org.apache.spark.ml.feature.StringIndexer = strIdx_db5100db888e\n",
       "indexed_v2: org.apache.spark.sql.DataFrame = [X1: string, X1_v2: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexer_v2 = new StringIndexer()\n",
    "    .setInputCols(Array(\"X1_v2\",\"X5_v2\"))\n",
    "    .setOutputCols(Array(\"X1_index\",\"X5_index\"))\n",
    "    .setStringOrderType(\"alphabetDesc\")\n",
    "\n",
    "val indexed_v2 = indexer_v2.fit(manufacturing_v2).transform(manufacturing_v2)\n",
    "\n",
    "indexed_v2\n",
    "    .select(\"X1_v2\",\"X1_index\",\"X5_v2\",\"X5_index\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T13:14:24.745187Z",
     "start_time": "2020-09-27T13:14:24.348712Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+---+--------+----------+\n",
      "| X1|X1_index|        X1_vec| X5|X5_index|    X5_vec|\n",
      "+---+--------+--------------+---+--------+----------+\n",
      "|  v|     0.0|(10,[0],[1.0])|  t|    10.0|(10,[],[])|\n",
      "|  b|     7.0|(10,[7],[1.0])|  b|    10.0|(10,[],[])|\n",
      "|  v|     0.0|(10,[0],[1.0])|  a|    10.0|(10,[],[])|\n",
      "|  l|     4.0|(10,[4],[1.0])|  z|    10.0|(10,[],[])|\n",
      "|  s|     1.0|(10,[1],[1.0])|  y|    10.0|(10,[],[])|\n",
      "| aa|     8.0|(10,[8],[1.0])|  x|    10.0|(10,[],[])|\n",
      "|  b|     7.0|(10,[7],[1.0])|  x|    10.0|(10,[],[])|\n",
      "|  s|     1.0|(10,[1],[1.0])|  h|    10.0|(10,[],[])|\n",
      "|  l|     4.0|(10,[4],[1.0])|  h|    10.0|(10,[],[])|\n",
      "|  v|     0.0|(10,[0],[1.0])|  g|    10.0|(10,[],[])|\n",
      "|  l|     4.0|(10,[4],[1.0])|  g|    10.0|(10,[],[])|\n",
      "|  b|     7.0|(10,[7],[1.0])|  g|    10.0|(10,[],[])|\n",
      "|  r|     2.0|(10,[2],[1.0])|  g|    10.0|(10,[],[])|\n",
      "|  v|     0.0|(10,[0],[1.0])|  g|    10.0|(10,[],[])|\n",
      "|  a|     9.0|(10,[9],[1.0])|  g|    10.0|(10,[],[])|\n",
      "|  r|     2.0|(10,[2],[1.0])|  g|    10.0|(10,[],[])|\n",
      "|  l|     4.0|(10,[4],[1.0])|  g|    10.0|(10,[],[])|\n",
      "|  r|     2.0|(10,[2],[1.0])|  f|    10.0|(10,[],[])|\n",
      "|  i|     5.0|(10,[5],[1.0])|  f|    10.0|(10,[],[])|\n",
      "|  r|     2.0|(10,[2],[1.0])|  f|    10.0|(10,[],[])|\n",
      "+---+--------+--------------+---+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "encoder_v2: org.apache.spark.ml.feature.OneHotEncoder = oneHotEncoder_2e13e432fcd4\n",
       "encoded_v2: org.apache.spark.sql.DataFrame = [X1: string, X1_v2: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val encoder_v2 = new OneHotEncoder()\n",
    "    .setInputCols(Array(\"X1_index\",\"X2_index\",\"X3_index\",\"X4_index\",\"X5_index\",\"X6_index\"))\n",
    "    .setOutputCols(Array(\"X1_vec\",\"X2_vec\",\"X3_vec\",\"X4_vec\",\"X5_vec\",\"X6_vec\"))\n",
    "\n",
    "val encoded_v2 = encoder.fit(indexed_v2).transform(indexed_v2)\n",
    "\n",
    "encoded_v2\n",
    "    .select(\"X1\",\"X1_index\",\"X1_vec\",\"X5\",\"X5_index\",\"X5_vec\")\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
